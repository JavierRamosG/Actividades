---
title: "Actividad Ayudantía 8 - Javier Ramos"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Creación de Playlist Spotify

## 1. Carga de data

### 1.1 Librerias
```{r}
library(quanteda)
library(dplyr)
library(tidyverse)
library(utf8)
library(cluster)
library(mclust)
library(e1071)
```

### 1.2 Base de datos
```{r}
load("D:/UAI/2021 - 1/Minería de datos/Actividades/Actividad Ayudantía 8/beats.RData")
```
  
## 2. Preprocesamiento de datos

### 2.1 Eliminar data

Se procederá a eliminar aquellos datos que no se utilizarán en el análisis
```{r}
beats <- beats[,!(colnames(beats) %in% c("artist_id", "album_id", "album_type","album_release_date", "album_release_date_precision", "analysis_url", "disc_number", "explicit", "track_href", "is_local", "track_preview_url", "track_number", "type", "track_uri", "external_urls.spotify", "album_name", "key_mode", "mode_name", "key_name", "time_signature", "album_release_year"))]
```

### 2.2 Eliminar datos vacíos

Se procederá a eliminar aquellas entidades que contengan atributos vacíos
```{r}
beats[beats == ""] <- NA
beats <- na.omit(beats)
```

## 2.3 Eliminar datos duplicados

Se eliminarán aquellas canciones que estén duplicadas
```{r}
beats <- beats[!duplicated(beats$track_id),]

```


### 2.4 Revisar estructura de los datos

Se transformará cada variable a su tipo correspondiente
```{r}
beats$track_id <- as.character(beats$track_id)
beats$track_name <- as.character(beats$track_name)
beats$artist_name <- as.character(beats$artist_name)

beats$danceability <- as.double(as.character(beats$danceability))
beats$energy <- as.double(as.character(beats$energy))
beats$key <- as.double(as.character(beats$key))
beats$loudness <- as.double(as.character(beats$loudness))
beats$mode <- as.double(as.character(beats$mode))
beats$speechiness <- as.double(as.character(beats$speechiness)) 
beats$acousticness <- as.double(as.character(beats$acousticness))
beats$instrumentalness <- as.double(as.character(beats$instrumentalness))
beats$liveness <- as.double(as.character(beats$liveness))
beats$valence <- as.double(as.character(beats$valence))
beats$tempo <- as.double(as.character(beats$tempo))
beats$duration_ms <- as.double(as.character(beats$duration_ms))
```

### 2.5 Selección aleatoria

Se seleccionarán datos de forma aleatoria para poder ocupar menos recursos computacionales
```{r}
set.seed(500)

spBeats <- beats[sample(nrow(beats), 8000),]

```

### 2.6 Separar datos

Se separarán los datos para trabajar con variables de una misma "familia", esto es, de un mismo tipo
```{r}
dt_char <- c("track_id", "track_name", "artist_name")

dt_num <-c("key", "danceability", "energy", "loudness", "mode", "speechiness","acousticness","instrumentalness", "liveness", "valence", "tempo", "duration_ms")

data_num <- spBeats %>% 
  select(dt_num)
data_char <- spBeats %>% 
  select(dt_char)
```

### 2.7 Escalar datos

Se procede a realizar el escalamiento de datos para su posterior análisis
```{r}
escal_data <- sapply(data_num, scale) %>% as.data.frame()
```

## 3. Procesamiento de datos

### 3.1 Análisis de claustering: DBSCAN

Ahora que está escalada la data, se aplicará el algoritmo de DBSCAN.
```{r}
library(dbscan)

set.seed(369)

model = dbscan(escal_data, eps = 1, minPts = 6)

model
```

El modelo genera 79 clusters basado en los parametros dados

```{r}
ggplot(escal_data, aes(valence, energy, color = factor(model$cluster))) + 
  geom_point(alpha = 0.3) 
```


### 3.2 Análisis de claustering: Fuzzy C Means

Se usará este algoritmo para contrastar con la información antes obtenida.
```{r}
modelo_c_means <- cmeans(escal_data,  79, m=2)

ggplot(escal_data, aes(valence, energy, color = factor(modelo_c_means$cluster))) + 
  geom_point(alpha = 0.3) 
```

Luego, todo se ve muy difuso, sin exisitr claridad en los grupos.

Para saber qué tan efectivo es lo aplicado anteriormente se calculará el Coeficiente de partición difusa (FPC)
```{r}
matriz <- modelo_c_means$membership%*%t(modelo_c_means$membership)

(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```
Luego, se tiene que con 79 clusters existe mucha variabilidad

### 3.3 Análisis de claustering: GMM

Se analizarán los clusters a través del algoritmo GGM
```{r}
model_gmm = Mclust(escal_data)

ggplot(escal_data) + 
  aes(x=valence, y=energy, color=factor(model_gmm$classification)) + 
  geom_point(alpha=1)

plot(model_gmm, what = "BIC")
```

```{r}
summary(model_gmm)
```


Se puede apreciar que el algoritmo indica que la cantidad correcta de clusters son 9.

## 4. Segundo intento

### 4.1 Seleccionar algunas columnas de la data

```{r}
dt_num1 <-c("danceability", "energy", "speechiness","acousticness","instrumentalness", "valence")

data_num1 <- spBeats %>% 
  select(dt_num1)
```

Ahora se escalará la data

```{r}
escal_data1 <- sapply(data_num1, scale) %>% as.data.frame()
```


### 4.1 Análisis de claustering 2: DBSCAN

Se empieza la segunda iteración con el algoritmo DBSCAN
```{r}
library(dbscan)

set.seed(369)

model1 = dbscan(escal_data1, eps = 0.5, minPts = 6)

model1
```
Con los parametros ingresados el algoritmo arroja que son 43 clusters. Gráficamente:

```{r}
ggplot(escal_data1, aes(valence, energy, color = factor(model1$cluster))) + 
  geom_point(alpha = 0.3) 
```


### 4.2 Análisis de claustering 2: Fuzzy C Means

```{r}
set.seed(369)

modelo_c_means1 <- cmeans(escal_data1,  7,m=1.5) 

modelo_c_means1$membership %>% head()

ggplot(escal_data1, aes(valence, energy, color = factor(modelo_c_means1$cluster))) + 
  geom_point(alpha = 0.3) 
```

```{r}
matriz <- modelo_c_means1$membership%*%t(modelo_c_means1$membership) # producto matricial

(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```
Como se puede ver, el nivel de variabilidad disminuyó bastante respecto a la iteración anterior

### 4.3 Análisis de claustering 2: GMM

```{r}
library(mclust)

set.seed(369)

model_gmm1 = Mclust(escal_data1)
```

```{r}
ggplot(escal_data1) + 
  aes(x=valence, y=energy, color=factor(model_gmm1$classification)) + 
  geom_point(alpha=1)


plot(model_gmm1, what = "BIC")
```

En este último resultado el valor de cluster es 9, pero antes dio 7. Por lo tanto, se hará un tercer intento.

## 4. Tercer intento

### 4.1 Análisis de claustering 3: DBSCAN

Ahora en este tercer intento se cambiarán los parámetros para poder determinar grupos más homgeneos

Iteración 1:
```{r}
set.seed(369)

model1 = dbscan(escal_data1, eps = 0.5, minPts = 7)

model1

ggplot(escal_data1, aes(valence, energy, color = factor(model1$cluster))) + 
  geom_point(alpha = 0.3) 
```
Iteración 2:
```{r}
set.seed(369)

model1 = dbscan(escal_data1, eps = 0.5, minPts = 8)

model1

ggplot(escal_data1, aes(valence, energy, color = factor(model1$cluster))) + 
  geom_point(alpha = 0.3) 
```
Iteración 3:
```{r}
set.seed(369)

model1 = dbscan(escal_data1, eps = 0.5, minPts = 9)

model1

ggplot(escal_data1, aes(valence, energy, color = factor(model1$cluster))) + 
  geom_point(alpha = 0.3) 
```
Iteración 4:
```{r}
set.seed(369)

model1 = dbscan(escal_data1, eps = 0.5, minPts = 10)

model1

ggplot(escal_data1, aes(valence, energy, color = factor(model1$cluster))) + 
  geom_point(alpha = 0.3) 
```

Iteración 5:
```{r}
set.seed(369)

model1 = dbscan(escal_data1, eps = 0.6, minPts = 10)

model1

ggplot(escal_data1, aes(valence, energy, color = factor(model1$cluster))) + 
  geom_point(alpha = 0.3) 
```

Iteración 6:
```{r}
set.seed(369)

model1 = dbscan(escal_data1, eps = 0.7, minPts = 10)

model1

ggplot(escal_data1, aes(valence, energy, color = factor(model1$cluster))) + 
  geom_point(alpha = 0.3) 
```

Pareciera que a medida que la cantidad de cluster disminuye, más homogeneo se ven los grupos. Para confirmar esto se continuará con los siguiente métodos.

### 4.1 Análisis de claustering 3: Fuzzy C Mean

Se iterará al igual que antes:

Iteración 1:
```{r}
set.seed(369)

modelo_c_means1 <- cmeans(escal_data1,  6,m=1.5) 

modelo_c_means1$membership %>% head()

ggplot(escal_data1, aes(valence, energy, color = factor(modelo_c_means1$cluster))) + 
  geom_point(alpha = 0.3) 
```
```{r}
matriz <- modelo_c_means1$membership%*%t(modelo_c_means1$membership) # producto matricial
(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```

Iteración 2:

```{r}
set.seed(369)

modelo_c_means1 <- cmeans(escal_data1,  5,m=1.5) 

modelo_c_means1$membership %>% head()

ggplot(escal_data1, aes(valence, energy, color = factor(modelo_c_means1$cluster))) + 
  geom_point(alpha = 0.3) 
```
```{r}
matriz <- modelo_c_means1$membership%*%t(modelo_c_means1$membership) # producto matricial
(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```

Iteración 3:
```{r}
set.seed(369)

modelo_c_means1 <- cmeans(escal_data1,  4,m=1.5) 

modelo_c_means1$membership %>% head()

ggplot(escal_data1, aes(valence, energy, color = factor(modelo_c_means1$cluster))) + 
  geom_point(alpha = 0.3) 
```
```{r}
matriz <- modelo_c_means1$membership%*%t(modelo_c_means1$membership) # producto matricial
(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```

Al observar este último resultado, se tiene que con 5 clusters hay menos variabilidad que con 4 o que con 6, por que este sería un punto de inflexión. Ahora se continuará iterando, pero los cambios se realizarán en el segundo parámetro.

Iteración 4

```{r}
set.seed(369)

modelo_c_means1 <- cmeans(escal_data1,  5,m=1.4) 

modelo_c_means1$membership %>% head()

ggplot(escal_data1, aes(valence, energy, color = factor(modelo_c_means1$cluster))) + 
  geom_point(alpha = 0.3) 
```
```{r}
matriz <- modelo_c_means1$membership%*%t(modelo_c_means1$membership) # producto matricial
(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```


Iteración 5:
```{r}
set.seed(369)

modelo_c_means1 <- cmeans(escal_data1,  5,m=1.3) 

modelo_c_means1$membership %>% head()

ggplot(escal_data1, aes(valence, energy, color = factor(modelo_c_means1$cluster))) + 
  geom_point(alpha = 0.3) 
```
```{r}
matriz <- modelo_c_means1$membership%*%t(modelo_c_means1$membership) # producto matricial
(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```

Iteración 6:
```{r}
set.seed(369)

modelo_c_means1 <- cmeans(escal_data1,  5,m=1.2) 

modelo_c_means1$membership %>% head()

ggplot(escal_data1, aes(valence, energy, color = factor(modelo_c_means1$cluster))) + 
  geom_point(alpha = 0.3) 
```
```{r}
matriz <- modelo_c_means1$membership%*%t(modelo_c_means1$membership) # producto matricial
(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```

Iteración 7:
```{r}
set.seed(369)

modelo_c_means1 <- cmeans(escal_data1,  5,m=1.1) 

modelo_c_means1$membership %>% head()

ggplot(escal_data1, aes(valence, energy, color = factor(modelo_c_means1$cluster))) + 
  geom_point(alpha = 0.3) 
```
```{r}
matriz <- modelo_c_means1$membership%*%t(modelo_c_means1$membership) # producto matricial
(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```

Iteración 7:
```{r}
set.seed(369)

modelo_c_means1 <- cmeans(escal_data1,  5,m=0.9) 

modelo_c_means1$membership %>% head()

ggplot(escal_data1, aes(valence, energy, color = factor(modelo_c_means1$cluster))) + 
  geom_point(alpha = 0.3) 
```
```{r}
matriz <- modelo_c_means1$membership%*%t(modelo_c_means1$membership) # producto matricial
(FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz))
```

Al poner como segundo parametro 1.0 el resultado es nulo, pero al pasar a 0.9 sí entrega un resultado que es menor a al valor encontrado con 1,1. Luego, bajo los parámetros de 5 cluster y 1.1, la variabilidad es muy baja, por lo que se estaría en presencia de un óptimo.

Luego, dado que sí es capaz de que visualmente sí es posible ver los grupos y considerando la baja variabilidad, entonces el número de cluster correcto es 5.

